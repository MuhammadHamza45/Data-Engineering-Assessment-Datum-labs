# Total Sales Revenue for January 2024: Sum of sales in January 2024.

from pyspark.sql.functions import to_date, col, sum
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType

# Define the schema and create DataFrame for Sales
schema = StructType([
    StructField("sale_id", IntegerType(), True),
    StructField("product_id", StringType(), True),
    StructField("sale_date", StringType(), True),  # Use StringType for the date initially
    StructField("amount", FloatType(), True),
    StructField("category_id", StringType(), True)
])

data = [
    (1, "P001", "2024-01-01", 100.00, "C1"),
    (2, "P002", "2024-01-05", 150.00, "C2"),
    (3, "P001", "2024-01-10", 100.00, "C1"),
    (4, "P003", "2024-01-15", 200.00, "C3"),
    (5, "P002", "2024-01-20", 150.00, "C2")
]

df = spark.createDataFrame(data, schema=schema)

# Convert sale_date col to date type
df = df.withColumn("sale_date", to_date(df["sale_date"], "yyyy-MM-dd"))
#df.show()

# Filter data for January 2024
filter_data = df.filter((col("sale_date") >= "2024-01-01") & (col("sale_date") <= "2024-01-31"))
#filter_data.show()

# Total sales revenue for January 2024
total_sale = filter_data.agg(sum("amount").alias("TotalSalesRevenue")).collect()[0]["TotalSalesRevenue"]

print(f"Total Sales Revenue for January 2024: {total_sale}")
